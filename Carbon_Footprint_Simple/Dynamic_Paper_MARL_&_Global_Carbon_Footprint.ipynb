{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dynamic Paper_MARL & Global Carbon Footprint_School of AI.ipynb.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Valdini/Carbon-Footprint-Multi-Agent-Reinforcement-Learning/blob/master/Dynamic_Paper_MARL_&_Global_Carbon_Footprint_School_of_AI_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "MSRtxohCRsrU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ">>># **Reducing the Global Carbon Footprint based on <br>Multi-Agent Reinforcement Learning (MARL)**\n",
        "\n",
        ">>![alt text](https://cdn.pixabay.com/photo/2017/02/13/17/37/climate-change-2063240_960_720.jpg)\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        ">>>>>>><br>Valentin Kahn, Research Fellow, School of AI\n",
        "\n",
        ">>>>>>>><br>Contact: valentin.kahn@gmail.com\n",
        "\n",
        ">>>>>>>>><br>November 29, 2018<br/>\n",
        "\n",
        "\n",
        ">>>>>>>>>><br>**Abstract**\n",
        "\n",
        "<br>This research paper intends to model the investment of groups of countries into carbon emission reductions based on a Mixed Markov Game setting, applying the principles of off-policy single-agent Reinforcement Learning to a multi-agent setting with a Markov Decision Process (MDP).\n",
        "The study shows that countries which are choosing their carbon emission reduction actions under the constraint of optimizing their and their partners’ mid-term economic benefit, achieve both higher cumulative rewards and higher reductions in their per capita CO2 consumption, than their counterparts. It also shows that action choices considering immediate and future rewards for the individual agents, as well as the cumulative reward of all agents, converge to large reductions in the carbon emission level per capita of these countries. Multi-agent reinforcement learning (MARL) bears important problem-solving potential by modelling economic and political decision makers in simulated environments.\n",
        "\n",
        "<br>**Keywords:** *reinforcement learning, game theory, climate change, global warming, global carbon emissions, global carbon footprint, multi-agent reinforcement learning, mixed markov games, markov decision process, q-learning, correlated equilibrium, temporal-difference learning* \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "zqQ9O6BDUGGl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ">>>>>>>>### **1\tClimate Change and Game Theory**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "The following chapter introduces evidence and issues of climate change. The chapter talks about root causes, potential solutions, as well as economics of climate change and global warming. It finally models climate change as a non-cooperative game with an inefficient Nash equilibrium and sets the stage for multi-agent reinforcement learning (MARL) as an approach to overcoming this Nash equilibrium.\n",
        "\n",
        "\n",
        ">>>>>>>>>#### <br>**1.1\tEvidence of Global Warming**\n",
        "\n",
        "This paper is not discussing the impact and consequences of global warming in detail and assumes that undertaking measures to preventing global warming is reasonable. According to Budd (2016), there are at least five indicators that the global climate is changing: the rise of the Earth’s temperature, the loss of Arctic sea ice, the increase in mean sea level over the last 100 years, the increase in the number of events of extreme rainfall and the increased level of carbon dioxide in the atmosphere.\n",
        "\n",
        ">>>>>>>>#### <br>**1.2\tRoot Causes of Global Warming**\n",
        "\n",
        "The reasons for man-made global warming lie in the well-known and -discussed greenhouse effect. So-called “heat-trapping gases” or “greenhouse gases” prevent the reflected thermal energy of the sun from escaping into space. While the existence of these gases is essential to sustain human life on earth, their abundance is a cause of global warming. And human activity has indeed led to increased concentration of heat-trapping gases in the atmosphere. Additionally, human-induced changes have reduced nature’s capacity to absorb these gases, for instance through deforestation (Kelman, 2015). The main heat-trapping gases are carbon dioxide (CO2), methane (CH4), nitrous oxide (N2O) and water vapor (H2O, UCS USA, 2017). According to the Intergovernmental Panel on Climate Change IPCC (2013), of all these gases, carbon dioxide both has contributed by far the most to climate change between 1750 and 2011 and by far resides the longest in the atmosphere. Global CO2 emissions are currently (2014) at 4.97 metric tons per capita, up from 3.1 metric tons in 1960. Additionally, the world’s population has grown substantially over said time period (The World Bank, 2018). This paper assumes the consumption of CO2 to be the main factor when it comes to human-made global warming and climate change and subsequently focuses on the reduction of said consumption. The “World Economic and Social Survey” by the United Nations in 2011 proposes to cap the CO2 emissions per capita at 3 tons until 2050 (United Nations, 2011). According to the Intergovernmental Panel on Climate Change (IPCC, 2018), carbon emission intensity needs to be drastically reduced in order for us to be able to cap global warming to 1.5-2°C until 2100, as set by the Paris Agreement (United Nations Framework Convention on Climate Change (UNFCCC), 2018). \n",
        "\n",
        "The Carbon Disclosure Project (CDP, 2017) released a report that attributes half of global industrial greenhouse gas emissions since 1751 (around 1 trillion tons of carbon dioxide) to not more than 100 fossil fuel extracting firms, most of them in the coal, oil or gas sectors. As these companies are bound to operate at locations and jurisdictions that contain large volumes of carbon-based natural resources, limiting or altering their operations (e.g. through large-scale abatement techniques, such as carbon capture and sequestration) and thus their emissions would be subject to regulations in these jurisdictions. However, the major driving force behind greenhouse gas emissions is not the production of fossil fuels, but the consumption thereof. The major consumers of fossil fuels and their products are large industrial and emerging economies, with China and the United States being responsible for about half of the world’s greenhouse gas emissions (Talbot, 2014). In order to counteract global warming, it is the responsibility of these economies to reduce consumption of fossil fuels, by either leveraging alternative energy sources (such as renewable energies) and/or reducing their economic output overall.\n",
        "\n",
        ">>>>>>>>#### <br>**1.3\tEconomics of Climate Change Mitigation**\n",
        "\n",
        "At this point in time, costs and benefits of reducing fossil fuel consumption cannot be estimated accurately. As emitted CO2 lasts about 100 years in the atmosphere, costly reduction measures with high uncertainty about timing and size of incurred benefits in the future seem hard to sell to taxpayers and large emitters today.\tHowever, there are large and immediate economic benefits from reducing CO2 emissions, such as a reduction in damages from air pollution, energy efficiency and impact on competitiveness (Hamilton, 2017). McKinsey’s greenhouse gas abatement curve is one of the more advanced approaches to estimate abatement cost with a time horizon until 2030. It shows the trade-off between cost and abatement potential. The model maps the impact of abatement measures (CO2 saved through each measure) with their net cost, limited to measures which do not cause more than 60 Euro in net cost per ton of CO2 eliminated (McKinsey & Company, 2013). Some of the measures (most of them related to energy efficiency or land management) show a positive economic outcome until 2030. In fact, the total economic cost of implementing all options of the model would be less than 1% of the global GDP by 2030, totaling 47 billion tons of CO2 saved per year (Ritchie, 2017). Additionally, most positive economic long-term effects are hard to estimate, such as the health impact of reducing air pollution in China. Also, it is likely that further technology advancements and innovations will reduce the cost of abatement (Zenghelis, 2018). However, the highest-impact measures are typically on the expensive side of the scale, combining an initially high capital intensity of investment with a decrease of operating costs spread over the following years. For all measures combined, upfront capital investments of 530 billion Euro per year by 2020 and 810 billion Euro per year by 2030 would be required (Ritchie, 2017).\n",
        "\n",
        "Given the benefits of CO2 reductions outweighing the cost and given the increased pressure on policy makers and carbon emitters – why is progress in CO2 reductions not taking place in an accelerated manner? Arguably, because the magnitude of benefits many times is unclear and the benefits cannot be distributed to one party (in particular, the party where the cost occurred), but the measures in general tend to benefit all actors, also passive ones, called “free-riders” (Traeger, 2009). \n",
        "\n",
        ">>>>>>>>#### <br>**1.4\tPolitics of Climate Change Mitigation**\n",
        "\n",
        "Entering the field of game theory might lead to a better understanding of the dynamics between the actors that are responsible for reducing CO2 emissions and thus should support in drawing conclusions on what results these actors might achieve depending on the reward maximization scheme they are applying. Dyke (2016) argues that climate change offers a good setup for a so-called “general-sum stochastic game”, with individual actors (be it countries, corporations, investors or consumers) depleting a common resource – the earth’s capacity to absorb CO2 – to their short-term individual financial benefit. There is thus a misalignment between incentives for the individual and the group. In game theory, a game in which individuals collectively deplete a common-pool resource is called the “tragedy of the commons”. The existence of this sort of game in climate change is currently fostered by the absence of a multinational regulatory system that would reward cooperation and punish free riding caused by a reward maximization scheme that is purely based on short-term self-interest (Dyke, 2016). \n",
        "\n",
        "While in former critical political situations, locally induced approaches to cooperation have shown over time to build trust between actors and enable large-scale cooperation and social norms that benefit the collective, the issue arising in climate change is the difference in magnitude and timeframe of the negative impact of climate change that individual nations and groups of nations face. While a developing country in the midst of the sea rather tends to face a strong negative impact in a mid-term scenario of further rising sea levels (one such example is Papua New Guinea), a landlocked and developed country with a moderate climate and a strongly service and industrial sector-oriented economic output might not face significant negative impact by climate change in the near future (Kelman, 2015). Additionally, the economic operations of these developed and industrialised countries tend to have the largest impact on the climate, with these countries remaining least affected in the mid-term (Dyke, 2016). In game theory, when all parties are to choose an action considering the probability of the actions that other parties take given their assumed reward maximization scheme, they will choose the action that maximizes their individual benefit, given the actions that they assume the other parties to take. When, in a non-cooperative game, neither party can change their action without foregoing some of their individual benefit, we call this a “Nash equilibrium”. In the case of climate negotiations, collectively postponing action has been the norm Nash equilibrium in the past and present, even though not being efficient (meaning not maximizing the cumulative benefit for all parties, Traeger, 2009). \n",
        "\n",
        "Previous policy attempts could not proceed past the soft law stage, leaving it open for the parties to not participate at all or to later decide to not comply with the rules (Traeger, 2009). As the effects of climate change move closer, non-Nash strategies, like local alliances and cooperation, need to enter main-stage in order to prevent further future damage (Mond, 2013; Traeger, 2009), given that global cooperation or law enforcement strategies have failed so far. Progressive policy making could also foster to break out of the Nash equilibrium, by making individual actions rational, for instance through a global fund that benefits those countries, corporations and subsequently individuals, that actively reduce CO2 emissions (Kenyon, 2018). Another approach is linking the commitment to international environmental soft law with the membership in important multinational organizations (like WTO or EU), such that free-riding parties are potentially excluded from these organizations, as proposed by Traeger (2009). Kelman (2015) criticizes that climate change is tackled separately from the other sustainable development goals (SDG), and that a more integrated and interdisciplinary policy approach needs to be implemented. \n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "1kPC5zJPVU22",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ">>>>### **2\tMulti-Agent Reinforcement Learning and Mixed Markov Games**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "In the following chapter, we will enter the world of reinforcement learning and explore how multi-agent reinforcement learning might help us to draw important insights to solve the problem of climate change.\n",
        "\n",
        ">>>>>>>#### <br>**2.1\tReinforcement Learning & Q-Learning**\n",
        "\n",
        "In reinforcement learning, an agent learns to behave in order to maximize its cumulative reward in an environment by choosing some action from a (finite or infinite) action space and given an initial state, receiving a reward and observing a subsequent transition of the state. Such a process is called a “Markov Decision Process” (MDP). The goal of an agent in an MDP is to maximize its discounted cumulative reward (defined by a reward function). By actively pursuing its goal, the agent either directly follows or subsequently creates a policy, which maps potential states to the actions that the agent is assumed to take to maximize its reward. While a deterministic policy maps a state to a single best action, a stationary policy assigns a probability distribution over actions to be taken to each state (Littman, 2000). In an environment that does not arrive with a pre-set policy, so-called “off-policy” methods can estimate the value of each action given a particular state, the so-called “action-value” (Busoniu et al., 2010).\n",
        "\n",
        "Q-Learning is a model-free, off-policy action-value estimation method that always converges to the optimal policy, based on a so-called “Q-table”, consisting of all possible states (rows) and actions (columns) in an environment, showing the result of the so-called “Q-function” for each given state-action pair. The Q-function, denoted as\n",
        "\t\t\t\t\n",
        ">>>>>>>$Q^\\ast\\left(s,a\\right)=r\\left(s,a\\right)+\\ \\gamma\\ \\ast\\ Q\\ast\\left(s\\prime,a\\right)$\n",
        "\n",
        "\n",
        "\n",
        ", calculates Q-values, representing the reward r of an action a in a given state s, plus the maximum reward achievable by the best action a’ in the next state s’, discounted by the factor gamma $\\gamma$ (temporal-difference learning). Q* denotes the highest Q-value for a given row, thus defines the action in a given state that achieves the highest Q-value (Valkov, 2017). \n",
        "\n",
        "The Q-table is updated after each iteration by\n",
        "\n",
        ">>>>>$Q(s,a)\\ =\\ (1-\\alpha)\\ \\ \\ast\\ \\ Q(s,a)\\ +\\ \\alpha\\ast\\ (r\\ +\\ \\gamma\\ \\ \\ast\\ {\\ Q}^\\ast(s\\prime,a))$\n",
        "\n",
        ", where $\\alpha$ denotes the learning rate of the algorithm and ${\\ Q}^\\ast(s\\prime,a)$ denotes the value of the next state as the maximal Q-value for the best action in the next state (Kansal & Martin, 2018).\n",
        "In order for the agent to not just exploit the best action based on previous observations and the learned Q-values, but also keep exploring the other options in order to receive Q-values for the whole action space, agents are adjusted to sometimes take randomized actions, as controlled by the parameter epsilon ε (Littman, 2000). \n",
        "\n",
        "\n",
        ">>>>>>>#### <br>**2.2\tMulti-Agent Reinforcement Learning**\n",
        "\n",
        "When multiple agents apply reinforcement learning in a shared environment, their optimal policies are additionally dependent from the other agents’ actions and policies (Nowé, 2014).\n",
        "Multi-agent Q-Learning (MultiQ) is the concept of extending the Q-function by not just mapping all possible states with all possible actions of one agent, but with the potential combination of actions of all agents in the game. Multi-agent Q-Learning thus faces the challenge of the “curse of dimensionality”, meaning an exponential increase of required storage and computing power for each additional modelled state and agent (Greenwald & Hall, 2003). An alternative for modelling multiple agents in a Markov Decision Process and approximating their optimal Q-values is to rely on single-agent Q-learning. In this scenario, the Q-functions for each agent are calculated separately, based on their own actions only. This means that the actions of the other agents do not directly influence the Q-values of a given agent. However, depending on the model, the shared state of the agents can influence the actions of the individual agents, thus leading to an indirect dependence between their actions, as all their actions impact the shared state (Busoniu et al., 2010).\n",
        "\n",
        "\n",
        ">>>>>>#### <br>**2.3\tMixed Markov Games & Correlated Equilibria**\n",
        "\n",
        "A game containing agents that are neither fully collaborative nor fully competitive is called a “Mixed Game”. A mixed game within a dynamic environment, where the agents optimize an MDP as independent learners and in a decentralized manner, is called a “Mixed Markov Game” (MG). As such, the MG is a general-sum game, meaning that receiving rewards is not mutually exclusive and thus the benefit of one agent is not necessarily to the disadvantage of the other agents (which would be a zero-sum game, Pérolat, 2016).\n",
        "\n",
        "While the pursuit of a Nash equilibrium is not always efficient (Traeger, 2009), the concept of a “correlated equilibrium” generalizes the Nash equilibrium and allows the agents to both make their decision dependent on the other agents’ decisions, but also to optimize according to their own reward function. Correlated-Q Learning applies the correlated equilibrium to Markov Games. While agents are still allowed to optimize their reward functions individually (based on single-agent Q-Learning), their reward function itself is subject to consideration of the other agents’ rewards in an equilibrium policy (Greenwald & Hall, 2003). Greenwald and Hall define four different equilibrium policies that each maximize the sum, maximum or minimum of either each of the individual agents’, some of the individual agents’ or the cumulative reward. An equilibrium policy that maximizes the sum of the agents’ rewards is called a “utilitarian policy” (Greenwald & Hall, 2003).\n"
      ]
    },
    {
      "metadata": {
        "id": "uasZ5Nb_ZC55",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ">>>>>>>>>### **3\tApproach**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "This study models the behavior of multiple agents representing groups of nation states in reducing their CO2 consumption while taking actions selfishly but not competitively, in a mixed Markov game setting with a global state and correlated reward functions and policies, modelled based on Coordination Equilibrium Q-Learning (CE-Q) reaching a utilitarian equilibrium, and reinforcement learning. The goal of the Markov game is to overcome the “Tragedy of the commons”, where multiple agents deplete a natural resource by purely maximizing their individual benefit (Dyke, 2016). The simulation is written in Python programming language, using only “NumPy” and “Random” as additional Python packages. The simulation is executed in Google Colab, which allows any spectator to run the simulation in their browser without having to install any dependencies.\n",
        "\n",
        "<br>The following simulation setup is chosen:\n",
        "\n",
        "\n",
        "*  The agents play a mixed Markov stochastic game with a fully observable environment and a known state transition function.\n",
        "*  There are three agents, defined as groups of countries. The first agent is not estimated to be immediately affected by the effects of global warming, but only in the longer term. Let us call it the “long-term impact agent” (LT). The second agent experiences a slight impact in the short-term and is equally affected in the longer term. Let us call it the “mid-term impact agent” (MT). The third agent is estimated to be already affected by the effects of global warming in the shorter term and equally affected in the longer term. Let us call it the “short-term impact agent” (ST).\n",
        "\n",
        "*   There is one global state which is initialized and defined as a small normalized scalar number representing the real global CO2 consumption of t0 (the starting year of the game). These are 4.97 tons CO2 per capita as of 2014, as defined by the World Bank (2018)\n",
        "\n",
        "*   The number of state transitions (between two states or “periods”), representing the number of years the game is played, is chosen by the spectator of the simulation. The spectator can choose between 1 and 30 years.\n",
        "\n",
        "*   The number of learning epochs of the algorithm is also defined by the spectator of the simulation. The spectator can choose between 1 and 50 learning epochs.\n",
        "\n",
        "*   The agents undertake parallel actions which are defined as the per capita reduction in the CO2 consumption of the agents. The action space is defined as (-0.2, -0.16, -0.12, -0.08, -0.04, 0, 0.04, 0.08, 0.12, 0.16, 0.2). LT has a higher (but non-greedy) tendency for exploitative actions, whereas ST has a higher tendency for explorative actions, as defined by their respective epsilon ε values. The actions are chosen from a finite action space. The epsilon values are decayed over time. This allows the agents to explore larger parts of the action space (Busoniu et al., 2010).\n",
        "*  The state transition function for a given period is defined as the global state before that period, minus the average per capita CO2 consumption reduction achieved by all agents in that period.\n",
        "\n",
        "\n",
        "*   The immediate rewards of the agents are defined based on the per capita CO2 consumption reduction achieved by the agent, multiplied by a reward factor (scalar number between 0 and 1), which varies depending on the agent, and subtracted by the cost of action, which is defined as the per capita CO2 consumption reduction and is multiplied by a cost factor, which is defined as 5.\n",
        "\n",
        "\n",
        ">*   LT receives a lower scalar reward factor, thus a smaller immediate reward. This is because the long-term agent does not necessarily feel the negative impact of climate change directly. Its motivation to invest in CO2 emission reductions – the agent considering mid- to long-term impact effects of investment decisions – is lower.\n",
        "\n",
        "\n",
        ">*   On the other end of the spectrum, ST receives a higher scalar reward factor, thus a higher immediate reward. This is because the short-term agent does feel the negative impact of climate change sooner. Its motivation to invest in CO2 emission reductions is higher.\n",
        "\n",
        "\n",
        "*   The discounted future reward of the next state is modelled based on Q-learning. The discount factor of the future reward γ is the same for all agents, given that all agents face the same long-term effects. The learning rate α is decayed over time (Bowling & Veloso, 2001).\n",
        "\n",
        "*   The cumulative reward is defined as the sum of the immediate rewards of all agents in the respective period, plus the sum of the per capita CO2 consumption reductions achieved by the agents in the respective period, multiplied by a scalar factor.\n",
        "*   Three different policies emerge from training the agents in the simulation – selfish, greedy and utilitarian policies. These policies are compared regarding the cumulative reward they achieve, as well as the per capita CO2 consumption reduction they achieve, expressed by the final global state in the end period. In the selfish policy, the agents choose those actions which they have learned to maximize the sum of their respective individual immediate and future rewards. Under greedy policies, the agents choose those actions that maximize their respective individual immediate rewards in the respective period. Finally, under utilitarian policies, the agents choose those actions which maximize the cumulative reward over all periods.\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "0-72uedainsP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "This simulation models investments by 3 agents (countries) into CO2 reductions\n",
        "based on game theory and Multi-Agent Reinforcement Learning (MARL). \n",
        "It complements a research paper written by Valentin Kahn for the School of AI\n",
        "Contact: valentin.kahn@gmail.com\n",
        "\n",
        "How to use:\n",
        "Run the following code in your browser by\n",
        "1) Signing in to your Google account\n",
        "2) Click on \"Open in Playground\" and \"Run anyway\"\n",
        "3) Click on the \">\" icon or by pressing Ctrl + Enter within this field\n",
        "4) Scroll to the bottom, and enter your choice of number of epochs and number of periods \n",
        "5) to see the simulation results at the bottom\n",
        "'''\n",
        "\n",
        "#importing dependencies\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "#defining the number of epochs and periods based on user input\n",
        "while True:\n",
        "    number_epochs = input(\"Enter the number of epochs (1-50) you would like to model: \")\n",
        "    try:\n",
        "       number_epochs = int(number_epochs)\n",
        "    except ValueError:\n",
        "       print ('Please enter a valid number between 1 and 50')\n",
        "       continue\n",
        "    if 0 <= number_epochs <= 51:\n",
        "       break\n",
        "    else:\n",
        "       print ('Please enter a valid number between 1 and 50')\n",
        "\n",
        "\n",
        "while True:\n",
        "    number_periods = input(\"Enter the number of periods (1-30) you would like to model: \")\n",
        "    try:\n",
        "       number_periods = int(number_periods)\n",
        "    except ValueError:\n",
        "       print ('Please enter a valid number between 1 and 30')\n",
        "       continue\n",
        "    if 0 <= number_periods <= 31:\n",
        "       break\n",
        "    else:\n",
        "       print ('Please enter a valid number between 1 and 30')\n",
        "\n",
        "#initializing all global variables\n",
        "#initializing variable that counts the state periods\n",
        "period = 0 \n",
        "number_of_periods = number_periods+2 #based on user input\n",
        "epoch = 1 #initializing variable that counts the epochs\n",
        "number_of_epochs = number_epochs #based on user input\n",
        "number_of_agents = 3\n",
        "\n",
        "#for graphical purposes; showing the action space in the first row of the Q-table\n",
        "action_count = (\"     -0.2 -0.16 -0.12 -0.08 -0.04  0    0.04  0.08  0.12  0.16  0.2\") \n",
        "\n",
        "global_state = 4.97 #current CO2 emissions are at 4.97 metric tons per capita world-wide. Source: World Bank\n",
        "\n",
        "#creating a list to store the global state for each epoch\n",
        "global_state_per_epoch = np.zeros((number_of_epochs, 2)) \n",
        "\n",
        "#number of different actions that a single agent can take in a given state\n",
        "size_of_action_space = 11 \n",
        "\n",
        "#action space contains of reducing or increasing the amount of tons CO2 consumed per capita\n",
        "action_space = [-0.2,-0.16,-0.12,-0.08,-0.04,0,0.04,0.08,0.12,0.16,0.2] \n",
        "action_index = [0,1,2,3,4,5,6,7,8,9,10]\n",
        "cost_of_action = 5 #defining cost to reduce CO2 emissions per metric ton\n",
        "\n",
        "#initializing the Q-tables\n",
        "Q_LT = np.zeros((number_of_periods-1, size_of_action_space)) \n",
        "Q_MT = np.zeros((number_of_periods-1, size_of_action_space)) \n",
        "Q_ST = np.zeros((number_of_periods-1, size_of_action_space)) \n",
        "\n",
        "#defining best actions from Q-tables\n",
        "Max_Q_LT = np.argmax(Q_LT[period, :])\n",
        "Max_Q_MT = np.argmax(Q_MT[period, :])\n",
        "Max_Q_ST = np.argmax(Q_ST[period, :])\n",
        "\n",
        "#further aid variable for analysis of cumulative rewards\n",
        "Q_LT_per_epoch = np.zeros((number_of_epochs, number_of_periods-1))\n",
        "Q_MT_per_epoch = np.zeros((number_of_epochs, number_of_periods-1))\n",
        "Q_ST_per_epoch = np.zeros((number_of_epochs, number_of_periods-1))\n",
        "\n",
        "#defining the weight factors of immediate rewards \n",
        "LT_reward_factor = 0.4\n",
        "MT_reward_factor = 0.5\n",
        "ST_reward_factor = 0.6 \n",
        "cumulative_reward = 0 #initializing cumulative reward, which is 0 to start with\n",
        "\n",
        "#creating a list to store the cumulative reward for each epoch\n",
        "cumulative_reward_per_epoch = np.zeros((number_of_epochs, 2)) \n",
        "\n",
        "#creating a list to store the immediate rewards for each epoch\n",
        "immediate_rewards_per_epoch = np.zeros((number_of_epochs, number_of_agents + 1)) \n",
        "\n",
        "LT_epsilon = 0.9 #epsilon for LT, variable for exploration vs. exploitation, higher epsilon = more exploitation\n",
        "LT_epsilon_min = 0.1 #defining minimal epsilon for LT\n",
        "LT_epsilon_decay = 0.95 #defining decay rate of LT's epsilon\n",
        "MT_epsilon = 0.8\n",
        "MT_epsilon_min = 0.06\n",
        "MT_epsilon_decay = 0.9\n",
        "ST_epsilon = 0.7 \n",
        "ST_epsilon_min = 0.03 \n",
        "ST_epsilon_decay = 0.85 \n",
        "alpha = 0.1 #initializing the learning rate of the Q-values\n",
        "alpha_min = 0.01 #initializing minimal learning rate after decay\n",
        "alpha_decay = 0.995 #initializing decay of learning rate\n",
        "gamma = 0.7  #initializing the reward discount; the higher gamma, the more value a future state creates\n",
        "\n",
        "#showing period numbers in first column of Q-table\n",
        "def Q_rowcount(Q_Table):\n",
        "  subcount = 1\n",
        "  for row in Q_Table:\n",
        "    count = f\"p{subcount}\"\n",
        "    print(count, row)\n",
        "    subcount += 1\n",
        "    \n",
        "\n",
        "#run epoch loop\n",
        "for epoch in range (1, number_of_epochs):\n",
        "  \n",
        "  #initializing variables which are reset for every new epoch\n",
        "  global_state = 4.97 #current CO2 emissions are at 4.97 metric tons per capita world-wide. Source: World Bank\n",
        "  cumulative_reward = 0 #initializing cumulative reward, which is 0 to start with\n",
        "  alpha = 0.1 #initializing the learning rate of the Q-values\n",
        "  \n",
        "  #initializing epsilons for each agent, the variable to balance exploration and exploitation, higher epsilon means more exploitation\n",
        "  LT_epsilon = 0.9 \n",
        "  MT_epsilon = 0.8 \n",
        "  ST_epsilon = 0.7 \n",
        "  \n",
        "  #run period loop\n",
        "  for period in range (0, number_of_periods-2):\n",
        "    \n",
        "    #initializing variables\n",
        "    random_action = random.choice(action_index)\n",
        "    LT_action = action_space[random_action]\n",
        "    random_action = random.choice(action_index)\n",
        "    MT_action = action_space[random_action]\n",
        "    random_action = random.choice(action_index)\n",
        "    ST_action = action_space[random_action]\n",
        "    \n",
        "    #Immediate Reward functions\n",
        "    LT_immediate_reward = -1 * LT_action * LT_reward_factor + cost_of_action * LT_action \n",
        "    MT_immediate_reward = -1 * MT_action * MT_reward_factor + cost_of_action * MT_action \n",
        "    ST_immediate_reward = -1 * ST_action * ST_reward_factor + cost_of_action * ST_action    \n",
        "    \n",
        "    #action iteration and Q-Table updates with exploration vs. exploitation\n",
        "    #LT\n",
        "    if np.random.rand() <= LT_epsilon:\n",
        "      random_action = random.choice(action_index)\n",
        "      LT_action = action_space[random_action]\n",
        "      Q_LT[period, random_action] = round((1-alpha) * Q_LT[period, random_action] + alpha * (LT_immediate_reward + gamma * np.amax(Q_LT[period + 1, :])), 2)\n",
        "    else:\n",
        "      LT_action = action_space[Max_Q_LT]\n",
        "      Q_LT[period, Max_Q_LT] = round((1-alpha) * Q_LT[period, Max_Q_LT] + alpha * (LT_immediate_reward + gamma * np.amax(Q_LT[period + 1, :])), 2)\n",
        "    \n",
        "    #MT\n",
        "    if np.random.rand() <= MT_epsilon:\n",
        "      random_action = random.choice(action_index)\n",
        "      MT_action = action_space[random_action]\n",
        "      Q_MT[period, random_action] = round((1-alpha) * Q_MT[period, random_action] + alpha * (MT_immediate_reward + gamma * np.amax(Q_MT[period + 1, :])), 2)\n",
        "    else:\n",
        "      MT_action = action_space[Max_Q_MT]\n",
        "      Q_MT[period, Max_Q_MT] = round((1-alpha) * Q_MT[period, Max_Q_MT] + alpha * (LT_immediate_reward + gamma * np.amax(Q_MT[period + 1, :])), 2)\n",
        "    \n",
        "    #ST\n",
        "    if np.random.rand() <= ST_epsilon:\n",
        "      random_action = random.choice(action_index)\n",
        "      ST_action = action_space[random_action]\n",
        "      Q_ST[period, random_action] = round((1-alpha) * Q_ST[period, random_action] + alpha * (ST_immediate_reward + gamma * np.amax(Q_ST[period + 1, :])), 2)\n",
        "    else:\n",
        "      ST_action = action_space[Max_Q_ST]\n",
        "      Q_ST[period, Max_Q_ST] = round((1-alpha) * Q_ST[period, Max_Q_ST] + alpha * (ST_immediate_reward + gamma * np.amax(Q_ST[period + 1, :])), 2)\n",
        "    \n",
        "    #to be maximized: += immediate rewards of both agents + the negative of the actions that each agent takes, because the actions influence the global state, which shall be minimized\n",
        "    cumulative_reward += LT_immediate_reward + MT_immediate_reward + ST_immediate_reward - 5*(action_space[np.argmax(Q_LT[period, :])]) - 5*(action_space[np.argmax(Q_MT[period, :])]) - 5*(action_space[np.argmax(Q_ST[period, :])])\n",
        "    \n",
        "    #decaying learning rate and agent's epsilon values\n",
        "    if alpha > alpha_min:\n",
        "      alpha *= alpha_decay\n",
        "    else:\n",
        "      alpha = alpha\n",
        "    if LT_epsilon > LT_epsilon_min:\n",
        "      LT_epsilon *= LT_epsilon_decay\n",
        "    else:\n",
        "      LT_epsilon = LT_epsilon\n",
        "    if MT_epsilon > MT_epsilon_min:\n",
        "      MT_epsilon *= MT_epsilon_decay\n",
        "    else:\n",
        "      MT_epsilon = MT_epsilon\n",
        "    if ST_epsilon > ST_epsilon_min:\n",
        "      ST_epsilon *= ST_epsilon_decay\n",
        "    else:\n",
        "      ST_epsilon = ST_epsilon\n",
        "    \n",
        "    #udpating global state and period counter\n",
        "    global_state += (LT_action + MT_action + ST_action) / number_of_agents\n",
        "    period +=1\n",
        "  \n",
        "  #for each epoch, store the policies of each agent\n",
        "  Q_LT_per_epoch[epoch, :] = np.vstack([np.argmax(Q_LT, axis=1).tolist()])\n",
        "  Q_MT_per_epoch[epoch, :] = np.vstack([np.argmax(Q_MT, axis=1).tolist()])\n",
        "  Q_ST_per_epoch[epoch, :] = np.vstack([np.argmax(Q_ST, axis=1).tolist()])\n",
        "  \n",
        "  #store the cumulative and immediate rewards and the global states of each epoch\n",
        "  cumulative_reward_per_epoch[epoch, :] = (epoch, cumulative_reward)\n",
        "  immediate_rewards_per_epoch[epoch, :] = (epoch, LT_immediate_reward, MT_immediate_reward, ST_immediate_reward)\n",
        "  global_state_per_epoch[epoch, :] = (epoch, global_state)\n",
        "  \n",
        "  epoch += 1\n",
        "  \n",
        "  \n",
        "\n",
        "#evaluating the trained model\n",
        "global_state_per_epoch[0][1] = 4.97\n",
        "print('\\n')\n",
        "print(f\"Best Epoch based on Cumulative Reward: {np.argmax(cumulative_reward_per_epoch[:, 1])}\")\n",
        "print(f\"Highest Cumulative Reward: {round(np.amax(cumulative_reward_per_epoch[:, 1]), 2)}\")\n",
        "print(f\"Best Epoch based on Global State: {np.argmin(global_state_per_epoch[:, 1])}\")\n",
        "print(f\"Lowest Global State: {round(np.min(global_state_per_epoch[:, 1]), 2)}\")\n",
        "print('\\n')\n",
        "print(f\"Best Epoch based on LT's Immediate Reward: {np.argmax(immediate_rewards_per_epoch[:, 1])}\")\n",
        "print(f\"Highest Immediate Reward for LT: {round(np.max(immediate_rewards_per_epoch[:, 1]), 2)}\")\n",
        "print(f\"Best Epoch based on MT's Immediate Reward: {np.argmax(immediate_rewards_per_epoch[:, 2])}\")\n",
        "print(f\"Highest Immediate Reward for MT: {round(np.max(immediate_rewards_per_epoch[:, 2]), 2)}\")\n",
        "print(f\"Best Epoch based on ST's Immediate Reward: {np.argmax(immediate_rewards_per_epoch[:, 3])}\")\n",
        "print(f\"Highest Immediate Reward for ST: {round(np.max(immediate_rewards_per_epoch[:, 3]), 2)}\")\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "#build utilitarian, selfish and greedy policies of each agent\n",
        "Q_LT_Best = Q_LT_per_epoch[np.argmax(cumulative_reward_per_epoch[:, 1]).tolist()]\n",
        "Q_MT_Best = Q_MT_per_epoch[np.argmax(cumulative_reward_per_epoch[:, 1]).tolist()]\n",
        "Q_ST_Best = Q_ST_per_epoch[np.argmax(cumulative_reward_per_epoch[:, 1]).tolist()]\n",
        "\n",
        "Q_LT_Immediate_Best = Q_LT_per_epoch[np.argmax(immediate_rewards_per_epoch[:, 1]).tolist()]\n",
        "Q_MT_Immediate_Best = Q_MT_per_epoch[np.argmax(immediate_rewards_per_epoch[:, 1]).tolist()]\n",
        "Q_ST_Immediate_Best = Q_ST_per_epoch[np.argmax(immediate_rewards_per_epoch[:, 2]).tolist()]\n",
        "                                                       \n",
        "LT_Strategy = [action_space[i] for i in Q_LT_Best.astype(int)[1:]]\n",
        "MT_Strategy = [action_space[i] for i in Q_MT_Best.astype(int)[1:]]\n",
        "ST_Strategy = [action_space[i] for i in Q_ST_Best.astype(int)[1:]]\n",
        "\n",
        "LT_Greedy_Strategy = [action_space[i] for i in Q_LT_Immediate_Best.astype(int)[1:]]\n",
        "MT_Greedy_Strategy = [action_space[i] for i in Q_MT_Immediate_Best.astype(int)[1:]]\n",
        "ST_Greedy_Strategy = [action_space[i] for i in Q_ST_Immediate_Best.astype(int)[1:]]\n",
        "\n",
        "LT_Policy = [action_space[i] for i in np.argmax(Q_LT, axis=1)]\n",
        "MT_Policy = [action_space[i] for i in np.argmax(Q_MT, axis=1)]\n",
        "ST_Policy = [action_space[i] for i in np.argmax(Q_ST, axis=1)]\n",
        "\n",
        "print(f\"LT's Strategy to achieve Highest Cumulative Reward: \\n {LT_Strategy}\")\n",
        "print(f\"LT's Strategy to achieve Highest Immediate Reward: \\n {LT_Greedy_Strategy}\")\n",
        "print(f\"Selfish Policy of LT, based on LT's Final Q-Table: \\n {LT_Policy}\")\n",
        "print('\\n')\n",
        "\n",
        "print(f\"MT's Strategy to achieve Highest Cumulative Reward: \\n {MT_Strategy}\")\n",
        "print(f\"MT's Strategy to achieve Highest Immediate Reward: \\n {MT_Greedy_Strategy}\")\n",
        "print(f\"Selfish Policy of MT, based on MT's Final Q-Table: \\n {MT_Policy}\")\n",
        "print('\\n')\n",
        "\n",
        "print(f\"ST's Strategy to achieve Highest Cumulative Reward: \\n {ST_Strategy}\")\n",
        "print(f\"ST's Strategy to achieve Highest Immediate Reward: \\n {ST_Greedy_Strategy}\")\n",
        "print(f\"Selfish Policy of ST, based on ST's Final Q-Table: \\n {ST_Policy}\")\n",
        "print('\\n')\n",
        "\n",
        "print(f\"Final Q-Table of LT: \\n {action_count}\") #Q-Tables are printed...\n",
        "print(Q_rowcount(Q_LT[:number_of_periods-2]))\n",
        "print('\\n')\n",
        "\n",
        "print(f\"Final Q-Table of MT: \\n {action_count}\") #Q-Tables are printed...\n",
        "print(Q_rowcount(Q_MT[:number_of_periods-2]))\n",
        "print('\\n')\n",
        "\n",
        "print(f\"Final Q-Table of ST: \\n {action_count}\") #...Based on the \"rewards_per_epoch\" Table, the best Q-Tables are identified as policies\n",
        "print(Q_rowcount(Q_ST[:number_of_periods-2]))\n",
        "\n",
        "\n",
        "\n",
        "#Under Selfish Policies\n",
        "#initializing variables which are reset for every new epoch\n",
        "global_state = 4.97 #current CO2 emissions are at 4.97 metric tons per capita world-wide. Source: World Bank\n",
        "cumulative_reward_selfish = 0 #initializing cumulative reward, which is 0 to start with\n",
        "alpha = 0.1 #initializing the learning rate of the Q-values\n",
        "Q_LT = np.zeros((number_of_periods-1, size_of_action_space))\n",
        "Q_MT = np.zeros((number_of_periods-1, size_of_action_space)) \n",
        "Q_ST = np.zeros((number_of_periods-1, size_of_action_space))\n",
        "  \n",
        "#run period loop\n",
        "for period in range (0, number_of_periods-2):\n",
        "  \n",
        "  #actions based on Policies\n",
        "  LT_action = LT_Policy[period]\n",
        "  LT_action = MT_Policy[period]\n",
        "  ST_action = ST_Policy[period]\n",
        "\n",
        "  #Immediate Reward functions\n",
        "  LT_immediate_reward = -1 * LT_action * LT_reward_factor + cost_of_action * LT_action #defining immediate reward function of LT per period\n",
        "  MT_immediate_reward = -1 * LT_action * MT_reward_factor + cost_of_action * MT_action #defining immediate reward function of LT per period\n",
        "  ST_immediate_reward = -1 * ST_action * ST_reward_factor + cost_of_action * ST_action #defining immediate reward function of ST per period    \n",
        "\n",
        "  #to be maximized: += immediate rewards of both agents + the negative of the actions that each agent takes, because the actions influence the global state, which shall be minimized\n",
        "  cumulative_reward_selfish += LT_immediate_reward + MT_immediate_reward + ST_immediate_reward - 5*(action_space[np.argmax(Q_LT[period, :])]) - 5*(action_space[np.argmax(Q_MT[period, :])]) - 5*(action_space[np.argmax(Q_ST[period, :])])\n",
        "  \n",
        "  global_state += (LT_action + MT_action + ST_action) / number_of_agents\n",
        "  period +=1\n",
        "\n",
        "print('\\n')\n",
        "print(f\"Cumulative Reward under Selfish Policies: {round(cumulative_reward_selfish, 2)}\")\n",
        "print(f\"Global State after using Selfish Policies: {round(global_state, 2)}\")\n",
        "\n",
        "Selfish_Reward_Loss = round(100*(cumulative_reward_selfish - np.max(cumulative_reward_per_epoch[:, 1])) / np.max(cumulative_reward_per_epoch[:, 1]),2)\n",
        "CO2_Selfish = round(100*((global_state - np.min(global_state_per_epoch[:, 1])) / np.min(global_state_per_epoch[:, 1])), 2)\n",
        "\n",
        "print('\\n')\n",
        "print(f\"Percentage of Lower Cumulative Reward comparing Selfish Policies to Best Epoch: {Selfish_Reward_Loss}%\")\n",
        "print(f\"Percentage of Higher Global State comparing Selfish Policies to Best Epoch: {CO2_Selfish}%\")\n",
        "\n",
        "\n",
        "\n",
        "#Under Greedy Policies\n",
        "#initializing variables which are reset for every new epoch\n",
        "global_state = 4.97 #current CO2 emissions are at 4.97 metric tons per capita world-wide. Source: World Bank\n",
        "cumulative_reward_greedy = 0 #initializing cumulative reward, which is 0 to start with\n",
        "alpha = 0.1 #initializing the learning rate of the Q-values\n",
        "  \n",
        "#run period loop\n",
        "for period in range (0, number_of_periods-2):\n",
        "  \n",
        "  #actions based on Policies\n",
        "  LT_action = LT_Greedy_Strategy[period]\n",
        "  MT_action = MT_Greedy_Strategy[period]\n",
        "  ST_action = ST_Greedy_Strategy[period]\n",
        "\n",
        "  #Immediate Reward functions\n",
        "  LT_immediate_reward = -1 * LT_action * LT_reward_factor + cost_of_action * LT_action \n",
        "  MT_immediate_reward = -1 * LT_action * MT_reward_factor + cost_of_action * MT_action \n",
        "  ST_immediate_reward = -1 * ST_action * ST_reward_factor + cost_of_action * ST_action    \n",
        "\n",
        "  #to be maximized: += immediate rewards of both agents + the negative of the actions that each agent takes, because the actions influence the global state, which shall be minimized\n",
        "  cumulative_reward_greedy += LT_immediate_reward + MT_immediate_reward  + ST_immediate_reward - 5*(action_space[np.argmax(Q_LT[period, :])]) - 5*(action_space[np.argmax(Q_MT[period, :])]) - 5*(action_space[np.argmax(Q_ST[period, :])])\n",
        "  \n",
        "  global_state += (LT_action + MT_action + ST_action) / number_of_agents\n",
        "  period +=1\n",
        "\n",
        "print('\\n')\n",
        "print(f\"Cumulative Reward under Greedy Policies: {round(cumulative_reward_greedy, 2)}\")\n",
        "print(f\"Global State after using Greedy Policies: {round(global_state, 2)}\")\n",
        "\n",
        "Greedy_Reward_Loss = round(100*(cumulative_reward_greedy - np.max(cumulative_reward_per_epoch[:, 1])) / np.max(cumulative_reward_per_epoch[:, 1]),2)\n",
        "CO2_Greedy = round(100*((global_state - np.min(global_state_per_epoch[:, 1])) / np.min(global_state_per_epoch[:, 1])), 2)\n",
        "\n",
        "print('\\n')\n",
        "print(f\"Percentage of Lower Cumulative Reward comparing Greedy Policies to Best Epoch: {Greedy_Reward_Loss}%\")\n",
        "print(f\"Percentage of Higher Global State comparing Greedy Policies to Best Epoch: {CO2_Greedy}%\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FP4oCh0sahQr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ">>>>>>>>>### **4\tResults & Discussion**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "The simulation is tested with six different total period lengths (5, 10, 15, 20, 25 and 30 \n",
        "periods), in 50 learning epochs and five test runs each. The following table sums up the final \n",
        "cumulative rewards and global states for the three different policies, “Utilitarian”, “Selfish” and \n",
        "“Greedy”, averaged over the five test runs per period configuration. The cumulative reward is \n",
        "defined by the cumulative reward function, the final global state is defined as the final average per \n",
        "capita CO2 consumption of all agents combined. \n",
        "\n",
        "<br> | <br>| <br>5 Periods| <br>10 Periods| <br>15 Periods| <br>20 Periods| <br>25 Periods| <br>30 Periods\n",
        "--- | ---\n",
        "Cumulative reward | Utilitarian| 11.04| 24.21| 34.19| 48.46| 64.37| 73.31\n",
        "Cumulative reward | Selfish | 7.44| 16.17| 16.87| 28.96| 25.64| 33.78\n",
        "Cumulative reward | Greedy| 10.19| 17.28| 23.02| 28.07| 39.43| 44.87\n",
        "Final global state | Utilitarian| 4.26|\t3.60|\t2.70|\t1.93|\t0.86|\t0.18\n",
        "Final global state | Selfish| 4.44|\t3.97|\t2.95|\t2.77|\t1.40|\t0.92\n",
        "Final global state | Greedy| 4.62|\t4.05|\t3.35|\t2.61|\t2.37|\t1.67\n",
        "\n",
        "\n",
        "<br>When looking at above-shown table, as well as the policies and the Q-tables of the three agents (samples can be found in the “Appendix” section), the following observations can be inferred.\n",
        "\n",
        "<br>*Cumulative reward*\n",
        "* The cumulative reward is highest in each period configuration when utilitarian policies are applied by the agents.\n",
        "* The difference in cumulative rewards between utilitarian policies and other policies grows with an increasing number of periods simulated.\n",
        "* Greedy policies consistently achieve higher cumulative rewards (with one exception) than selfish policies. \n",
        "* Selfish policies show an abnormal decrease in cumulative reward achieved when modelled for 20 and 25 periods, compared to the other period configurations.\n",
        "\n",
        "<br>*Final global state*\n",
        "* Utilitarian policies consistently achieve lower global states than the other policies. \n",
        "* Selfish policies consistently achieve lower global states (with one exception) than greedy policies.\n",
        "\n",
        "<br>*Policies and actions*\n",
        "* Utilitarian policies tend to imply more closer-to-maximum (“-0.2”) actions than the other policies, while selfish and greedy policies show similar tendencies.\n",
        "* All three types of policies of all agents show to converge towards the maximum action “-0.2” with an increasing number of periods applied.\n",
        "* Agent ST tends to consistently take more action than agent MT, which tends to take more action than agent LT.\n",
        "\n",
        "<br>*Q-tables*\n",
        "* For 20 and more periods modelled, the Q-tables of all agents show an increasing sparsity which is due to the increasing number of state(period-)-action pairs to be explored.\n"
      ]
    },
    {
      "metadata": {
        "id": "OYT8Yow7d_Kn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ">>>>>>>>>### **5\tConclusions & Future work**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "The results show that utilitarian policies achieve both higher cumulative rewards and a lower per capita CO2 consumption, than selfish and greedy policies. In order to realize utilitarian policies, the current soft law approach on international climate policy needs to be enhanced by local commitments of smaller groups of countries and increased political and economic pressure. The policies of the modelled agents show that significantly reducing the global per capita CO2 consumption is not an option, but a rational thing to do, both from an economic and ecological perspective. Bringing these two worlds together, the cost of carbon emissions needs to be internalized in our economic systems, to incentivize both direct polluters and consumers to rethink their CO2 choices. While developed, landlocked countries might not face the impact of climate change as much in the near term than a developing island like Papua New Guinea, they will have to face equal consequences in the further future. Multi-agent reinforcement learning can help us to model and compare the economic and political dynamics and implications of new approaches to climate policy.\n",
        "\n",
        "Future work could build on more sophisticated and thus realistic models, including the use of big data for function approximation and more sophisticated, sensitive and dynamic functions. Some of the potential areas of improvement include but are not limited to: the number and diversity of agents, the cost and variety of mitigation measures, the different cost and impact of these measures depending on which period they are applied to, the state transition function, mapping the global state (the global CO2 consumption) to its impact (the consequences of global warming and climate change), mapping this impact to the agent’s reward functions, the reward functions in general, modelling the environment in a partially observable MDP which includes the additional mapping from observations to states, more sophisticated models to change the parameters depending on time, the use of different MARL algorithms depending on the model (such as Team-Q, WoLF-PHC, SG-SP, different variations of CE-Q or Nash-Q, to name just a few), accounting for models of cooperation between agents and finally, modelling real-time parallelization of and communication between agents (e.g. through using osBrain in the case of Python). \n",
        "\n",
        "This study has intended to show how Multi-Agent Reinforcement Learning (MARL) and game theory can be applied to explain the current issues in climate change negotiations and CO2 consumption reduction and how this currently and increasingly popular AI technology can contribute to solving these issues in the future. The study has also intended to symbolize that reinforcement learning can be applied beyond its undoubtedly important application area of physical games and simulations and that once combined with game theory, it has a great potential for achieving groundbreaking insights about human cooperation and competition, and thus for economic and political decision making.\n"
      ]
    },
    {
      "metadata": {
        "id": "NM8pdv7ReJxj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ">>>>>>>>### **6\tAcknowledgements**\n",
        "\n",
        "---\n",
        "\n",
        "The author thanks the School of AI and its Director, Siraj Raval, for their support, as well as Kushal Sharma for his help and contributions.\n",
        "\n",
        "\n",
        ">>>>>>>>>### **References**\n",
        "\n",
        "---\n",
        "\n",
        "Bowling, M., & Veloso, M. (2001, August). Rational and convergent learning in stochastic games. In International joint conference on artificial intelligence (Vol. 17, No. 1, pp. 1021-1026). Lawrence Erlbaum Associates Ltd.\n",
        "\n",
        "<br>Budd, C. (2016). Climate change: Does it all add up? https://plus.maths.org/content/climate-change-does-it-all-add\n",
        "\n",
        "<br>Busoniu, L., Babuska, R., & De Schutter, B. (2010). Multi-agent reinforcement learning: ˇAn overview\n",
        "\n",
        "<br>Carbon Disclosure Project (CDP, 2017). The Carbon Majors Database - Carbon Majors Report 2017. https://b8f65cb373b1b7b15feb-c70d8ead6ced550b4d987d7c03fcdd1d.ssl.cf3.rackcdn.com/cms/reports/documents/000/002/327/original/Carbon-Majors-Report-2017.pdf?1499691240\n",
        "\n",
        "<br>Dyke, J. (2016). Can game theory help solve the problem of climate change? https://www.theguardian.com/science/blog/2016/apr/13/can-game-theory-help-solve-the-problem-of-climate-change. The Guardian\n",
        "\n",
        "<br>Greenwald, A., Hall, K., & Serrano, R. (2003, August). Correlated Q-learning. In ICML (Vol. 3, pp. 242-249).\n",
        "\n",
        "<br>Hamilton, K. (2017). Economic co-benefits of reducing CO2 emissions outweigh the cost of mitigation for most big emitters. http://www.lse.ac.uk/GranthamInstitute/news/economic-co-benefits-of-reducing-co2-emissions-outweigh-the-cost-of-mitigation-for-most-big-emitters/. The London School of Economics and Political Science – Grantham Research Institute on Climate Change and the Environment\n",
        "\n",
        "<br>Intergovernmental Panel on Climate Change (IPCC, 2018). Less Than 2 °C Warming by 2100 Unlikely. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6070153/\n",
        "\n",
        "<br>Intergovernmental Panel on Climate Change (IPCC, 2013). Fifth Assessment Report (AR5). https://www.ipcc.ch/report/ar5/\n",
        "\n",
        "<br>Kansal, S., & Martin, B. (2018). Reinforcement Q-Learning from Scratch in Python with OpenAI Gym. https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/\n",
        "\n",
        "<br>Kelman, I. (2015). Joint action on climate change: Facts and figures. https://www.scidev.net/global/climate-change/feature/joint-action-climate-change-facts-figures.html\n",
        "\n",
        "<br>Kenyon, R. (2018). How to break the climate change Nash equilibrium. https://medium.com/nori-carbon-removal/how-to-break-the-climate-change-nash-equilibrium-f4ca3354cb8b\n",
        "\n",
        "<br>Littman, M. L. (2001). Value-function reinforcement learning in Markov games. Cognitive Systems Research, 2(1), 55-66.\n",
        "\n",
        "<br>McKinsey & Company (2013). Pathways to a Low-Carbon Economy. https://www.cbd.int/financial/doc/Pathwaystoalowcarboneconomy.pdf\n",
        "\n",
        "<br>Mond, D. (2013). Game Theory and Climate Change. http://homepages.warwick.ac.uk/~masbm/ClimateCourse/MondTalks/climategameunpause.pdf. University of Warwick, Mathematics Institute\n",
        "\n",
        "<br>Nowé, A., Vrancx, P., & De Hauwere, Y. M. (2012). Game theory and multi-agent reinforcement learning. In Reinforcement Learning (pp. 441-470). Springer, Berlin, Heidelberg.\n",
        "\n",
        "<br>Pérolat, J., Strub, F., Piot, B., & Pietquin, O. (2016). Learning Nash Equilibrium for General-Sum Markov Games from Batch Data. arXiv preprint arXiv:1606.08718.\n",
        "\n",
        "<br>Ritchie, H. (2017). How much will it cost to mitigate climate change? https://ourworldindata.org/how-much-will-it-cost-to-mitigate-climate-change. Our World in Data.\n",
        "\n",
        "<br>Talbot, D. (2014). Carbon Sequestration: Too Little, Too Late? https://www.technologyreview.com/s/531531/carbon-sequestration-too-little-too-late/\n",
        "\n",
        "<br>The World Bank (2018). CO2 emissions (metric tons per capita). https://data.worldbank.org/indicator/EN.ATM.CO2E.PC?end=2014&start=1960&view=chart&year_high_desc=true\n",
        "\n",
        "<br>Traeger, C. (2009). The economics of climate change. International Cooperation and Climate Policy. https://are.berkeley.edu/~traeger/Lectures/ClimateChangeEconomics/Slides/7%20International%20Cooperation%20-%201%20A%20Game%20Theoretic%20Perspective.pdf. UC Berkeley\n",
        "\n",
        "<br>Union of Concerned Scientists UCS USA (2017). CO2 is the issue: https://www.ucsusa.org/global-warming/science-and-impacts/science/CO2-and-global-warming-faq.html#.W_aH42hKg2x\n",
        "\n",
        "<br>United Nations Framework Convention on Climate Change (UNFCCC) (2018). https://unfccc.int/process/the-paris-agreement/what-is-the-paris-agreement\n",
        "\n",
        "<br>United Nations (2011). The Paris Agreement. World Economic and Social Survey 2011. http://www.un.org/en/development/desa/policy/wess/wess_current/2011wess_chapter2.pdf\n",
        "\n",
        "<br>Valkov, V. (2017). Solving an MDP with Q-Learning from scratch. https://medium.com/@curiousily/solving-an-mdp-with-q-learning-from-scratch-deep-reinforcement-learning-for-hackers-part-1-45d1d360c120\n",
        "\n",
        "<br>Zenghelis, D. (2018). How much will it cost to cut global greenhouse gas emissions? http://www.lse.ac.uk/GranthamInstitute/faqs/how-much-will-it-cost-to-cut-global-greenhouse-gas-emissions/. The London School of Economics and Political Science – Grantham Research Institute on Climate Change and the Environment\n"
      ]
    },
    {
      "metadata": {
        "id": "N2ggv0K2fQcX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ">>>>>>>>>>### **A Appendix**\n",
        "\n",
        "---\n",
        ">>>>>>>>>#### **A.1\tLink to Source Code**\n",
        "#####[Source Code on Colab](https://colab.research.google.com/drive/1wgy766JmPyX8wQhr2pyZpKRfyEpvH6no)"
      ]
    }
  ]
}