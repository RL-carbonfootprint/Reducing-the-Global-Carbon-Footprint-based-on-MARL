{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Source Code_MARL & Global Carbon Footprint_School of AI.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Valdini/Carbon-Footprint-Multi-Agent-Reinforcement-Learning/blob/master/Source_Code_MARL_&_Global_Carbon_Footprint_School_of_AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "l4rjA4lJhmEv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "This simulation models investments by 3 agents (countries) into CO2 reductions\n",
        "based on game theory and Multi-Agent Reinforcement Learning (MARL). \n",
        "It complements a research paper written by Valentin Kahn for the School of AI\n",
        "Contact: valentin.kahn@gmail.com\n",
        "\n",
        "How to use:\n",
        "Run the following code in your browser by\n",
        "1) Signing in to your Google account\n",
        "2) Click on \"Open in Playground\" and \"Run anyway\"\n",
        "3) Click on the \">\" icon or by pressing Ctrl + Enter within this field\n",
        "4) Scroll to the bottom, and enter your choice of number of epochs and number of periods \n",
        "5) to see the simulation results at the bottom\n",
        "'''\n",
        "\n",
        "#importing dependencies\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "#defining the number of epochs and periods based on user input\n",
        "while True:\n",
        "    number_epochs = input(\"Enter the number of epochs (1-50) you would like to model: \")\n",
        "    try:\n",
        "       number_epochs = int(number_epochs)\n",
        "    except ValueError:\n",
        "       print ('Please enter a valid number between 1 and 50')\n",
        "       continue\n",
        "    if 0 <= number_epochs <= 51:\n",
        "       break\n",
        "    else:\n",
        "       print ('Please enter a valid number between 1 and 50')\n",
        "\n",
        "\n",
        "while True:\n",
        "    number_periods = input(\"Enter the number of periods (1-30) you would like to model: \")\n",
        "    try:\n",
        "       number_periods = int(number_periods)\n",
        "    except ValueError:\n",
        "       print ('Please enter a valid number between 1 and 30')\n",
        "       continue\n",
        "    if 0 <= number_periods <= 31:\n",
        "       break\n",
        "    else:\n",
        "       print ('Please enter a valid number between 1 and 30')\n",
        "\n",
        "#initializing all global variables\n",
        "#initializing variable that counts the state periods\n",
        "period = 0 \n",
        "number_of_periods = number_periods+2 #based on user input\n",
        "epoch = 1 #initializing variable that counts the epochs\n",
        "number_of_epochs = number_epochs #based on user input\n",
        "number_of_agents = 3\n",
        "\n",
        "#for graphical purposes; showing the action space in the first row of the Q-table\n",
        "action_count = (\"     -0.2 -0.16 -0.12 -0.08 -0.04  0    0.04  0.08  0.12  0.16  0.2\") \n",
        "\n",
        "global_state = 4.97 #current CO2 emissions are at 4.97 metric tons per capita world-wide. Source: World Bank\n",
        "\n",
        "#creating a list to store the global state for each epoch\n",
        "global_state_per_epoch = np.zeros((number_of_epochs, 2)) \n",
        "\n",
        "#number of different actions that a single agent can take in a given state\n",
        "size_of_action_space = 11 \n",
        "\n",
        "#action space contains of reducing or increasing the amount of tons CO2 consumed per capita\n",
        "action_space = [-0.2,-0.16,-0.12,-0.08,-0.04,0,0.04,0.08,0.12,0.16,0.2] \n",
        "action_index = [0,1,2,3,4,5,6,7,8,9,10]\n",
        "cost_of_action = 5 #defining cost to reduce CO2 emissions per metric ton\n",
        "\n",
        "#initializing the Q-tables\n",
        "Q_LT = np.zeros((number_of_periods-1, size_of_action_space)) \n",
        "Q_MT = np.zeros((number_of_periods-1, size_of_action_space)) \n",
        "Q_ST = np.zeros((number_of_periods-1, size_of_action_space)) \n",
        "\n",
        "#defining best actions from Q-tables\n",
        "Max_Q_LT = np.argmax(Q_LT[period, :])\n",
        "Max_Q_MT = np.argmax(Q_MT[period, :])\n",
        "Max_Q_ST = np.argmax(Q_ST[period, :])\n",
        "\n",
        "#further aid variable for analysis of cumulative rewards\n",
        "Q_LT_per_epoch = np.zeros((number_of_epochs, number_of_periods-1))\n",
        "Q_MT_per_epoch = np.zeros((number_of_epochs, number_of_periods-1))\n",
        "Q_ST_per_epoch = np.zeros((number_of_epochs, number_of_periods-1))\n",
        "\n",
        "#defining the weight factors of immediate rewards \n",
        "LT_reward_factor = 0.4\n",
        "MT_reward_factor = 0.5\n",
        "ST_reward_factor = 0.6 \n",
        "cumulative_reward = 0 #initializing cumulative reward, which is 0 to start with\n",
        "\n",
        "#creating a list to store the cumulative reward for each epoch\n",
        "cumulative_reward_per_epoch = np.zeros((number_of_epochs, 2)) \n",
        "\n",
        "#creating a list to store the immediate rewards for each epoch\n",
        "immediate_rewards_per_epoch = np.zeros((number_of_epochs, number_of_agents + 1)) \n",
        "\n",
        "LT_epsilon = 0.9 #epsilon for LT, variable for exploration vs. exploitation, higher epsilon = more exploitation\n",
        "LT_epsilon_min = 0.1 #defining minimal epsilon for LT\n",
        "LT_epsilon_decay = 0.95 #defining decay rate of LT's epsilon\n",
        "MT_epsilon = 0.8\n",
        "MT_epsilon_min = 0.06\n",
        "MT_epsilon_decay = 0.9\n",
        "ST_epsilon = 0.7 \n",
        "ST_epsilon_min = 0.03 \n",
        "ST_epsilon_decay = 0.85 \n",
        "alpha = 0.1 #initializing the learning rate of the Q-values\n",
        "alpha_min = 0.01 #initializing minimal learning rate after decay\n",
        "alpha_decay = 0.995 #initializing decay of learning rate\n",
        "gamma = 0.7  #initializing the reward discount; the higher gamma, the more value a future state creates\n",
        "\n",
        "#showing period numbers in first column of Q-table\n",
        "def Q_rowcount(Q_Table):\n",
        "  subcount = 1\n",
        "  for row in Q_Table:\n",
        "    count = f\"p{subcount}\"\n",
        "    print(count, row)\n",
        "    subcount += 1\n",
        "    \n",
        "\n",
        "#run epoch loop\n",
        "for epoch in range (1, number_of_epochs):\n",
        "  \n",
        "  #initializing variables which are reset for every new epoch\n",
        "  global_state = 4.97 #current CO2 emissions are at 4.97 metric tons per capita world-wide. Source: World Bank\n",
        "  cumulative_reward = 0 #initializing cumulative reward, which is 0 to start with\n",
        "  alpha = 0.1 #initializing the learning rate of the Q-values\n",
        "  \n",
        "  #initializing epsilons for each agent, the variable to balance exploration and exploitation, higher epsilon means more exploitation\n",
        "  LT_epsilon = 0.9 \n",
        "  MT_epsilon = 0.8 \n",
        "  ST_epsilon = 0.7 \n",
        "  \n",
        "  #run period loop\n",
        "  for period in range (0, number_of_periods-2):\n",
        "    \n",
        "    #initializing variables\n",
        "    random_action = random.choice(action_index)\n",
        "    LT_action = action_space[random_action]\n",
        "    random_action = random.choice(action_index)\n",
        "    MT_action = action_space[random_action]\n",
        "    random_action = random.choice(action_index)\n",
        "    ST_action = action_space[random_action]\n",
        "    \n",
        "    #Immediate Reward functions\n",
        "    LT_immediate_reward = -1 * LT_action * LT_reward_factor + cost_of_action * LT_action \n",
        "    MT_immediate_reward = -1 * MT_action * MT_reward_factor + cost_of_action * MT_action \n",
        "    ST_immediate_reward = -1 * ST_action * ST_reward_factor + cost_of_action * ST_action    \n",
        "    \n",
        "    #action iteration and Q-Table updates with exploration vs. exploitation\n",
        "    #LT\n",
        "    if np.random.rand() <= LT_epsilon:\n",
        "      random_action = random.choice(action_index)\n",
        "      LT_action = action_space[random_action]\n",
        "      Q_LT[period, random_action] = round((1-alpha) * Q_LT[period, random_action] + alpha * (LT_immediate_reward + gamma * np.amax(Q_LT[period + 1, :])), 2)\n",
        "    else:\n",
        "      LT_action = action_space[Max_Q_LT]\n",
        "      Q_LT[period, Max_Q_LT] = round((1-alpha) * Q_LT[period, Max_Q_LT] + alpha * (LT_immediate_reward + gamma * np.amax(Q_LT[period + 1, :])), 2)\n",
        "    \n",
        "    #MT\n",
        "    if np.random.rand() <= MT_epsilon:\n",
        "      random_action = random.choice(action_index)\n",
        "      MT_action = action_space[random_action]\n",
        "      Q_MT[period, random_action] = round((1-alpha) * Q_MT[period, random_action] + alpha * (MT_immediate_reward + gamma * np.amax(Q_MT[period + 1, :])), 2)\n",
        "    else:\n",
        "      MT_action = action_space[Max_Q_MT]\n",
        "      Q_MT[period, Max_Q_MT] = round((1-alpha) * Q_MT[period, Max_Q_MT] + alpha * (LT_immediate_reward + gamma * np.amax(Q_MT[period + 1, :])), 2)\n",
        "    \n",
        "    #ST\n",
        "    if np.random.rand() <= ST_epsilon:\n",
        "      random_action = random.choice(action_index)\n",
        "      ST_action = action_space[random_action]\n",
        "      Q_ST[period, random_action] = round((1-alpha) * Q_ST[period, random_action] + alpha * (ST_immediate_reward + gamma * np.amax(Q_ST[period + 1, :])), 2)\n",
        "    else:\n",
        "      ST_action = action_space[Max_Q_ST]\n",
        "      Q_ST[period, Max_Q_ST] = round((1-alpha) * Q_ST[period, Max_Q_ST] + alpha * (ST_immediate_reward + gamma * np.amax(Q_ST[period + 1, :])), 2)\n",
        "    \n",
        "    #to be maximized: += immediate rewards of both agents + the negative of the actions that each agent takes, because the actions influence the global state, which shall be minimized\n",
        "    cumulative_reward += LT_immediate_reward + MT_immediate_reward + ST_immediate_reward - 5*(action_space[np.argmax(Q_LT[period, :])]) - 5*(action_space[np.argmax(Q_MT[period, :])]) - 5*(action_space[np.argmax(Q_ST[period, :])])\n",
        "    \n",
        "    #decaying learning rate and agent's epsilon values\n",
        "    if alpha > alpha_min:\n",
        "      alpha *= alpha_decay\n",
        "    else:\n",
        "      alpha = alpha\n",
        "    if LT_epsilon > LT_epsilon_min:\n",
        "      LT_epsilon *= LT_epsilon_decay\n",
        "    else:\n",
        "      LT_epsilon = LT_epsilon\n",
        "    if MT_epsilon > MT_epsilon_min:\n",
        "      MT_epsilon *= MT_epsilon_decay\n",
        "    else:\n",
        "      MT_epsilon = MT_epsilon\n",
        "    if ST_epsilon > ST_epsilon_min:\n",
        "      ST_epsilon *= ST_epsilon_decay\n",
        "    else:\n",
        "      ST_epsilon = ST_epsilon\n",
        "    \n",
        "    #udpating global state and period counter\n",
        "    global_state += (LT_action + MT_action + ST_action) / number_of_agents\n",
        "    period +=1\n",
        "  \n",
        "  #for each epoch, store the policies of each agent\n",
        "  Q_LT_per_epoch[epoch, :] = np.vstack([np.argmax(Q_LT, axis=1).tolist()])\n",
        "  Q_MT_per_epoch[epoch, :] = np.vstack([np.argmax(Q_MT, axis=1).tolist()])\n",
        "  Q_ST_per_epoch[epoch, :] = np.vstack([np.argmax(Q_ST, axis=1).tolist()])\n",
        "  \n",
        "  #store the cumulative and immediate rewards and the global states of each epoch\n",
        "  cumulative_reward_per_epoch[epoch, :] = (epoch, cumulative_reward)\n",
        "  immediate_rewards_per_epoch[epoch, :] = (epoch, LT_immediate_reward, MT_immediate_reward, ST_immediate_reward)\n",
        "  global_state_per_epoch[epoch, :] = (epoch, global_state)\n",
        "  \n",
        "  epoch += 1\n",
        "  \n",
        "  \n",
        "\n",
        "#evaluating the trained model\n",
        "global_state_per_epoch[0][1] = 4.97\n",
        "print('\\n')\n",
        "print(f\"Best Epoch based on Cumulative Reward: {np.argmax(cumulative_reward_per_epoch[:, 1])}\")\n",
        "print(f\"Highest Cumulative Reward: {round(np.amax(cumulative_reward_per_epoch[:, 1]), 2)}\")\n",
        "print(f\"Best Epoch based on Global State: {np.argmin(global_state_per_epoch[:, 1])}\")\n",
        "print(f\"Lowest Global State: {round(np.min(global_state_per_epoch[:, 1]), 2)}\")\n",
        "print('\\n')\n",
        "print(f\"Best Epoch based on LT's Immediate Reward: {np.argmax(immediate_rewards_per_epoch[:, 1])}\")\n",
        "print(f\"Highest Immediate Reward for LT: {round(np.max(immediate_rewards_per_epoch[:, 1]), 2)}\")\n",
        "print(f\"Best Epoch based on MT's Immediate Reward: {np.argmax(immediate_rewards_per_epoch[:, 2])}\")\n",
        "print(f\"Highest Immediate Reward for MT: {round(np.max(immediate_rewards_per_epoch[:, 2]), 2)}\")\n",
        "print(f\"Best Epoch based on ST's Immediate Reward: {np.argmax(immediate_rewards_per_epoch[:, 3])}\")\n",
        "print(f\"Highest Immediate Reward for ST: {round(np.max(immediate_rewards_per_epoch[:, 3]), 2)}\")\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "#build utilitarian, selfish and greedy policies of each agent\n",
        "Q_LT_Best = Q_LT_per_epoch[np.argmax(cumulative_reward_per_epoch[:, 1]).tolist()]\n",
        "Q_MT_Best = Q_MT_per_epoch[np.argmax(cumulative_reward_per_epoch[:, 1]).tolist()]\n",
        "Q_ST_Best = Q_ST_per_epoch[np.argmax(cumulative_reward_per_epoch[:, 1]).tolist()]\n",
        "\n",
        "Q_LT_Immediate_Best = Q_LT_per_epoch[np.argmax(immediate_rewards_per_epoch[:, 1]).tolist()]\n",
        "Q_MT_Immediate_Best = Q_MT_per_epoch[np.argmax(immediate_rewards_per_epoch[:, 1]).tolist()]\n",
        "Q_ST_Immediate_Best = Q_ST_per_epoch[np.argmax(immediate_rewards_per_epoch[:, 2]).tolist()]\n",
        "                                                       \n",
        "LT_Strategy = [action_space[i] for i in Q_LT_Best.astype(int)[1:]]\n",
        "MT_Strategy = [action_space[i] for i in Q_MT_Best.astype(int)[1:]]\n",
        "ST_Strategy = [action_space[i] for i in Q_ST_Best.astype(int)[1:]]\n",
        "\n",
        "LT_Greedy_Strategy = [action_space[i] for i in Q_LT_Immediate_Best.astype(int)[1:]]\n",
        "MT_Greedy_Strategy = [action_space[i] for i in Q_MT_Immediate_Best.astype(int)[1:]]\n",
        "ST_Greedy_Strategy = [action_space[i] for i in Q_ST_Immediate_Best.astype(int)[1:]]\n",
        "\n",
        "LT_Policy = [action_space[i] for i in np.argmax(Q_LT, axis=1)]\n",
        "MT_Policy = [action_space[i] for i in np.argmax(Q_MT, axis=1)]\n",
        "ST_Policy = [action_space[i] for i in np.argmax(Q_ST, axis=1)]\n",
        "\n",
        "print(f\"LT's Strategy to achieve Highest Cumulative Reward: \\n {LT_Strategy}\")\n",
        "print(f\"LT's Strategy to achieve Highest Immediate Reward: \\n {LT_Greedy_Strategy}\")\n",
        "print(f\"Selfish Policy of LT, based on LT's Final Q-Table: \\n {LT_Policy}\")\n",
        "print('\\n')\n",
        "\n",
        "print(f\"MT's Strategy to achieve Highest Cumulative Reward: \\n {MT_Strategy}\")\n",
        "print(f\"MT's Strategy to achieve Highest Immediate Reward: \\n {MT_Greedy_Strategy}\")\n",
        "print(f\"Selfish Policy of MT, based on MT's Final Q-Table: \\n {MT_Policy}\")\n",
        "print('\\n')\n",
        "\n",
        "print(f\"ST's Strategy to achieve Highest Cumulative Reward: \\n {ST_Strategy}\")\n",
        "print(f\"ST's Strategy to achieve Highest Immediate Reward: \\n {ST_Greedy_Strategy}\")\n",
        "print(f\"Selfish Policy of ST, based on ST's Final Q-Table: \\n {ST_Policy}\")\n",
        "print('\\n')\n",
        "\n",
        "print(f\"Final Q-Table of LT: \\n {action_count}\") #Q-Tables are printed...\n",
        "print(Q_rowcount(Q_LT[:number_of_periods-2]))\n",
        "print('\\n')\n",
        "\n",
        "print(f\"Final Q-Table of MT: \\n {action_count}\") #Q-Tables are printed...\n",
        "print(Q_rowcount(Q_MT[:number_of_periods-2]))\n",
        "print('\\n')\n",
        "\n",
        "print(f\"Final Q-Table of ST: \\n {action_count}\") #...Based on the \"rewards_per_epoch\" Table, the best Q-Tables are identified as policies\n",
        "print(Q_rowcount(Q_ST[:number_of_periods-2]))\n",
        "\n",
        "\n",
        "\n",
        "#Under Selfish Policies\n",
        "#initializing variables which are reset for every new epoch\n",
        "global_state = 4.97 #current CO2 emissions are at 4.97 metric tons per capita world-wide. Source: World Bank\n",
        "cumulative_reward_selfish = 0 #initializing cumulative reward, which is 0 to start with\n",
        "alpha = 0.1 #initializing the learning rate of the Q-values\n",
        "Q_LT = np.zeros((number_of_periods-1, size_of_action_space))\n",
        "Q_MT = np.zeros((number_of_periods-1, size_of_action_space)) \n",
        "Q_ST = np.zeros((number_of_periods-1, size_of_action_space))\n",
        "  \n",
        "#run period loop\n",
        "for period in range (0, number_of_periods-2):\n",
        "  \n",
        "  #actions based on Policies\n",
        "  LT_action = LT_Policy[period]\n",
        "  LT_action = MT_Policy[period]\n",
        "  ST_action = ST_Policy[period]\n",
        "\n",
        "  #Immediate Reward functions\n",
        "  LT_immediate_reward = -1 * LT_action * LT_reward_factor + cost_of_action * LT_action #defining immediate reward function of LT per period\n",
        "  MT_immediate_reward = -1 * LT_action * MT_reward_factor + cost_of_action * MT_action #defining immediate reward function of LT per period\n",
        "  ST_immediate_reward = -1 * ST_action * ST_reward_factor + cost_of_action * ST_action #defining immediate reward function of ST per period    \n",
        "\n",
        "  #to be maximized: += immediate rewards of both agents + the negative of the actions that each agent takes, because the actions influence the global state, which shall be minimized\n",
        "  cumulative_reward_selfish += LT_immediate_reward + MT_immediate_reward + ST_immediate_reward - 5*(action_space[np.argmax(Q_LT[period, :])]) - 5*(action_space[np.argmax(Q_MT[period, :])]) - 5*(action_space[np.argmax(Q_ST[period, :])])\n",
        "  \n",
        "  global_state += (LT_action + MT_action + ST_action) / number_of_agents\n",
        "  period +=1\n",
        "\n",
        "print('\\n')\n",
        "print(f\"Cumulative Reward under Selfish Policies: {round(cumulative_reward_selfish, 2)}\")\n",
        "print(f\"Global State after using Selfish Policies: {round(global_state, 2)}\")\n",
        "\n",
        "Selfish_Reward_Loss = round(100*(cumulative_reward_selfish - np.max(cumulative_reward_per_epoch[:, 1])) / np.max(cumulative_reward_per_epoch[:, 1]),2)\n",
        "CO2_Selfish = round(100*((global_state - np.min(global_state_per_epoch[:, 1])) / np.min(global_state_per_epoch[:, 1])), 2)\n",
        "\n",
        "print('\\n')\n",
        "print(f\"Percentage of Lower Cumulative Reward comparing Selfish Policies to Best Epoch: {Selfish_Reward_Loss}%\")\n",
        "print(f\"Percentage of Higher Global State comparing Selfish Policies to Best Epoch: {CO2_Selfish}%\")\n",
        "\n",
        "\n",
        "\n",
        "#Under Greedy Policies\n",
        "#initializing variables which are reset for every new epoch\n",
        "global_state = 4.97 #current CO2 emissions are at 4.97 metric tons per capita world-wide. Source: World Bank\n",
        "cumulative_reward_greedy = 0 #initializing cumulative reward, which is 0 to start with\n",
        "alpha = 0.1 #initializing the learning rate of the Q-values\n",
        "  \n",
        "#run period loop\n",
        "for period in range (0, number_of_periods-2):\n",
        "  \n",
        "  #actions based on Policies\n",
        "  LT_action = LT_Greedy_Strategy[period]\n",
        "  MT_action = MT_Greedy_Strategy[period]\n",
        "  ST_action = ST_Greedy_Strategy[period]\n",
        "\n",
        "  #Immediate Reward functions\n",
        "  LT_immediate_reward = -1 * LT_action * LT_reward_factor + cost_of_action * LT_action \n",
        "  MT_immediate_reward = -1 * LT_action * MT_reward_factor + cost_of_action * MT_action \n",
        "  ST_immediate_reward = -1 * ST_action * ST_reward_factor + cost_of_action * ST_action    \n",
        "\n",
        "  #to be maximized: += immediate rewards of both agents + the negative of the actions that each agent takes, because the actions influence the global state, which shall be minimized\n",
        "  cumulative_reward_greedy += LT_immediate_reward + MT_immediate_reward  + ST_immediate_reward - 5*(action_space[np.argmax(Q_LT[period, :])]) - 5*(action_space[np.argmax(Q_MT[period, :])]) - 5*(action_space[np.argmax(Q_ST[period, :])])\n",
        "  \n",
        "  global_state += (LT_action + MT_action + ST_action) / number_of_agents\n",
        "  period +=1\n",
        "\n",
        "print('\\n')\n",
        "print(f\"Cumulative Reward under Greedy Policies: {round(cumulative_reward_greedy, 2)}\")\n",
        "print(f\"Global State after using Greedy Policies: {round(global_state, 2)}\")\n",
        "\n",
        "Greedy_Reward_Loss = round(100*(cumulative_reward_greedy - np.max(cumulative_reward_per_epoch[:, 1])) / np.max(cumulative_reward_per_epoch[:, 1]),2)\n",
        "CO2_Greedy = round(100*((global_state - np.min(global_state_per_epoch[:, 1])) / np.min(global_state_per_epoch[:, 1])), 2)\n",
        "\n",
        "print('\\n')\n",
        "print(f\"Percentage of Lower Cumulative Reward comparing Greedy Policies to Best Epoch: {Greedy_Reward_Loss}%\")\n",
        "print(f\"Percentage of Higher Global State comparing Greedy Policies to Best Epoch: {CO2_Greedy}%\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}