{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Valdini/Carbon-Footprint-Multi-Agent-Reinforcement-Learning/blob/master/Source_Code_MARL_&_Global_Carbon_Footprint_School_of_AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l4rjA4lJhmEv"
   },
   "outputs": [],
   "source": [
    "#importing dependencies\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "#loading dataFrames\n",
    "LT_df = pd.read_csv('LT_db.csv', index_col=0)\n",
    "MT_df = pd.read_csv('MT_db.csv', index_col=0)\n",
    "ST_df = pd.read_csv('ST_db.csv', index_col=0)\n",
    "\n",
    "years_array=np.arange(int(LT_df.columns[0])-1,int(LT_df.columns[-1])+1)\n",
    "global_trend_Real = [4.025, 4.074, 4.124, 4.152, 4.227, 4.224, 4.194, 4.173, 4.068, 4.002,4.011, 4.036, 4.071, 4.082, 4.05, 3.968, 4.038, 4.081, 4.088, 4.258, 4.414,  4.528, 4.636, 4.671, 4.762, 4.662, 4.835, 4.975, 5.005, 4.998, 4.981]\n",
    "years = len(LT_df.columns)\n",
    "#initializing all global variables\n",
    "#initializing variable that counts the state periods\n",
    "epochs = 32\n",
    "number_of_agents = 3\n",
    "\n",
    "#for graphical purposes; showing the action space in the first row of the Q-table\n",
    "action_count = (\"     -0.2 -0.16 -0.12 -0.08 -0.04  0    0.04  0.08  0.12  0.16  0.2\") \n",
    "\n",
    "global_state = 4.97 #current CO2 emissions are at 4.97 metric tons per capita world-wide. Source: World Bank\n",
    "\n",
    "#creating a list to store the global state for each epoch\n",
    "global_state_per_epoch = []\n",
    "\n",
    "#number of different actions that a single agent can take in a given state\n",
    "size_of_action_space = 10\n",
    "cost_of_action = 10 #defining cost to reduce CO2 emissions per metric ton\n",
    "\n",
    "#Q-tables\n",
    "Q_LT = np.zeros((years + 1, size_of_action_space))\n",
    "Q_MT = np.zeros((years + 1, size_of_action_space))\n",
    "Q_ST = np.zeros((years + 1, size_of_action_space))\n",
    "\n",
    "#Q-tables for every epoch that stores all max values from\n",
    "Q_LT_per_epoch = []\n",
    "Q_MT_per_epoch = []\n",
    "Q_ST_per_epoch = []\n",
    "\n",
    "#defining the weight factors of immediate rewards \n",
    "LT_reward_factor = 0.4\n",
    "MT_reward_factor = 0.5\n",
    "ST_reward_factor = 0.6 \n",
    "cumulative_reward = 0 #initializing cumulative reward, which is 0 to start with\n",
    "\n",
    "#creating a list to store the cumulative reward for each epoch\n",
    "cumulative_reward_per_epoch = []\n",
    "\n",
    "#creating a list to store the immediate rewards for each epoch\n",
    "immediate_rewards_per_epoch = []\n",
    "\n",
    "LT_epsilon_min = 0.1 #defining minimal epsilon for LT\n",
    "LT_epsilon_decay = 0.999 #defining decay rate of LT's epsilon\n",
    "MT_epsilon_min = 0.06\n",
    "MT_epsilon_decay = 0.995\n",
    "ST_epsilon_min = 0.03 \n",
    "ST_epsilon_decay = 0.990 \n",
    "alpha = 0.1 #initializing the learning rate of the Q-values\n",
    "alpha_min = 0.01 #initializing minimal learning rate after decay\n",
    "alpha_decay = 0.999 #initializing decay of learning rate\n",
    "gamma = 0.7  #<=>reward discount\n",
    "LT_values = LT_df.values[:-1].T\n",
    "Population_LT = LT_df.values[-1]\n",
    "\n",
    "LT_ravel = LT_values.ravel()\n",
    "LT_avg = sum(LT_ravel) / len(LT_ravel)\n",
    "\n",
    "LT_action_space = np.round(\n",
    "    np.concatenate(\n",
    "        (np.linspace(min(LT_ravel), LT_avg, size_of_action_space // 2),\n",
    "         np.linspace(LT_avg, max(LT_ravel),\n",
    "                     size_of_action_space - (size_of_action_space // 2)))), 3)\n",
    "MT_values = MT_df.values[:-1].T\n",
    "Population_MT = MT_df.values[-1]\n",
    "\n",
    "MT_ravel = MT_values.ravel()\n",
    "MT_avg = sum(MT_ravel) / len(MT_ravel)\n",
    "\n",
    "MT_action_space = np.round(\n",
    "    np.concatenate(\n",
    "        (np.linspace(min(MT_ravel), MT_avg, size_of_action_space // 2),\n",
    "         np.linspace(MT_avg, max(MT_ravel),\n",
    "                     size_of_action_space - (size_of_action_space // 2)))), 3)\n",
    "\n",
    "ST_values = ST_df.values[:-1].T\n",
    "Population_ST = ST_df.values[-1]\n",
    "Population_array = Population_LT + Population_MT + Population_ST\n",
    "\n",
    "ST_ravel = ST_values.ravel()\n",
    "ST_avg = sum(ST_ravel) / len(ST_ravel)\n",
    "\n",
    "ST_action_space = np.round(\n",
    "    np.concatenate(\n",
    "        (np.linspace(min(ST_ravel), ST_avg, size_of_action_space // 2),\n",
    "         np.linspace(ST_avg, max(ST_ravel),\n",
    "                     size_of_action_space - (size_of_action_space // 2)))), 3)\n",
    "\n",
    "#defining best actions from action space\n",
    "Min_Q_LT = 0\n",
    "Min_Q_MT = 0\n",
    "Min_Q_ST = 0\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "\n",
    "    #initalize variables\n",
    "    global_state_of_co2_emission = 4.025  #CO2 emmision in 1984\n",
    "\n",
    "    cumulative_reward = 0  #intially 0\n",
    "\n",
    "    alpha = 0.1  #learning rate\n",
    "    \n",
    "    #exploration vs explotation\n",
    "    LT_epsilon = 0.9\n",
    "    MT_epsilon = 0.8\n",
    "    ST_epsilon = 0.7\n",
    "\n",
    "    for year in range(0, years):\n",
    "\n",
    "        Min_Q_LT = np.argmin(Q_LT[year])\n",
    "        Min_Q_MT = np.argmin(Q_MT[year])\n",
    "        Min_Q_ST = np.argmin(Q_ST[year])\n",
    "\n",
    "        #LT\n",
    "        if np.random.rand() <= LT_epsilon:\n",
    "            LT_action = random.choice(LT_values[year])\n",
    "        else:\n",
    "            LT_action = LT_action_space[Min_Q_LT]\n",
    "\n",
    "        LT_immediate_reward = LT_action * (LT_reward_factor + cost_of_action)\n",
    "        \n",
    "        Q_LT[year, abs(LT_action_space - LT_action).argmin()] = round(\n",
    "            (1 - alpha) * Q_LT[year, Min_Q_LT] + alpha *\n",
    "            (LT_immediate_reward + gamma * np.amin(Q_LT[year + 1, :])), 3)\n",
    "\n",
    "        #MT\n",
    "        if np.random.rand() <= MT_epsilon:\n",
    "            MT_action = random.choice(MT_values[year])\n",
    "        else:\n",
    "            MT_action = MT_action_space[Min_Q_MT]\n",
    "\n",
    "        MT_immediate_reward = MT_action * (MT_reward_factor + cost_of_action)\n",
    "        \n",
    "        Q_MT[year, abs(MT_action_space - MT_action).argmin()] = round(\n",
    "            (1 - alpha) * Q_MT[year, Min_Q_MT] + alpha *\n",
    "            (MT_immediate_reward + gamma * np.amin(Q_MT[year + 1, :])), 3)\n",
    "\n",
    "        #ST\n",
    "        if np.random.rand() <= ST_epsilon:\n",
    "            ST_action = random.choice(ST_values[year])\n",
    "        else:\n",
    "            ST_action = ST_action_space[Min_Q_ST]\n",
    "\n",
    "        ST_immediate_reward = ST_action * (ST_reward_factor + cost_of_action)\n",
    "        \n",
    "        Q_ST[year, abs(ST_action_space - ST_action).argmin()] = round(\n",
    "            (1 - alpha) * Q_ST[year, Min_Q_ST] + alpha *\n",
    "            (ST_immediate_reward + gamma * np.amin(Q_ST[year + 1, :])), 3)\n",
    "\n",
    "        cumulative_reward += LT_immediate_reward + MT_immediate_reward + ST_immediate_reward - cost_of_action * (\n",
    "            LT_action_space[np.argmin(Q_LT[year, :])] -\n",
    "            MT_action_space[np.argmin(Q_MT[year, :])] -\n",
    "            ST_action_space[np.argmin(Q_ST[year, :])])\n",
    "\n",
    "        alpha = alpha * alpha_decay if (alpha > alpha_min) else alpha\n",
    "        \n",
    "        LT_epsilon = LT_epsilon * LT_epsilon_decay if (\n",
    "            LT_epsilon > LT_epsilon_min) else LT_epsilon_min\n",
    "        \n",
    "        MT_epsilon = MT_epsilon * MT_epsilon_decay if (\n",
    "            MT_epsilon > MT_epsilon_min) else MT_epsilon_min\n",
    "        \n",
    "        ST_epsilon = ST_epsilon * ST_epsilon_decay if (\n",
    "            ST_epsilon > ST_epsilon_min) else ST_epsilon_min\n",
    "\n",
    "        global_state_of_co2_emission += (\n",
    "            (LT_action * Population_LT[year]) +\n",
    "            (MT_action * Population_MT[year]) +\n",
    "            (ST_action * Population_ST[year])) / Population_array[year]\n",
    "\n",
    "    Q_LT_per_epoch.append(np.argmin(Q_LT, axis=1).tolist())\n",
    "    Q_MT_per_epoch.append(np.argmin(Q_MT, axis=1).tolist())\n",
    "    Q_ST_per_epoch.append(np.argmin(Q_ST, axis=1).tolist())\n",
    "\n",
    "    cumulative_reward_per_epoch.append(cumulative_reward)\n",
    "    immediate_rewards_per_epoch.append(\n",
    "        [LT_immediate_reward, MT_immediate_reward, ST_immediate_reward])\n",
    "    global_state_per_epoch.append(global_state_of_co2_emission)\n",
    "    \n",
    "immediate_rewards_per_epoch = np.array(immediate_rewards_per_epoch, copy=False)\n",
    "#evaluating the trained model\n",
    "print('Lowest Global State Achived During this Game: ',np.min(global_state_per_epoch))\n",
    "print('\\n')\n",
    "print(\"Lowest Immediate Reward for LT: \",np.min(immediate_rewards_per_epoch[:, 0]))\n",
    "print(\"Lowest Immediate Reward for MT: \",np.min(immediate_rewards_per_epoch[:, 1]))\n",
    "print(\"Lowest Immediate Reward for ST: \",np.min(immediate_rewards_per_epoch[:, 2]))\n",
    "print('Q_LT: \\n')\n",
    "print(pd.DataFrame(Q_LT).head())\n",
    "print('Q_MT: \\n')\n",
    "print(pd.DataFrame(Q_MT).head())\n",
    "print('Q_ST: \\n')\n",
    "print(pd.DataFrame(Q_ST).head())\n",
    "\n",
    "#Synergistic Policy\n",
    "Q_LT_Best = Q_LT_per_epoch[np.argmin(cumulative_reward_per_epoch)]\n",
    "Q_MT_Best = Q_MT_per_epoch[np.argmin(cumulative_reward_per_epoch)]\n",
    "Q_ST_Best = Q_ST_per_epoch[np.argmin(cumulative_reward_per_epoch)]\n",
    "\n",
    "LT_Synergistic = [LT_action_space[i] for i in Q_LT_Best[1:]]\n",
    "MT_Synergistic = [MT_action_space[i] for i in Q_MT_Best[1:]]\n",
    "ST_Synergistic = [ST_action_space[i] for i in Q_ST_Best[1:]]\n",
    "\n",
    "print(\"LT's Strategy to achieve Lowest Cumulative Reward:\\n\",LT_Synergistic)\n",
    "print(\"\\nMT's Strategy to achieve Lowest Cumulative Reward:\\n\",MT_Synergistic)\n",
    "print(\"\\nST's Strategy to achieve Lowest Cumulative Reward:\\n\",ST_Synergistic)\n",
    "\n",
    "#Selfish Planning Policy\n",
    "Q_LT_Immediate_Best = Q_LT_per_epoch[np.argmin(immediate_rewards_per_epoch[:, 0])]\n",
    "Q_MT_Immediate_Best = Q_MT_per_epoch[np.argmin(immediate_rewards_per_epoch[:, 1])]\n",
    "Q_ST_Immediate_Best = Q_ST_per_epoch[np.argmin(immediate_rewards_per_epoch[:, 2])]\n",
    "\n",
    "\n",
    "LT_Selfish_Plan = [LT_action_space[i] for i in Q_LT_Immediate_Best[1:]]\n",
    "MT_Selfish_Plan = [MT_action_space[i] for i in Q_MT_Immediate_Best[1:]]\n",
    "ST_Selfish_Plan = [ST_action_space[i] for i in Q_ST_Immediate_Best[1:]]\n",
    "\n",
    "print(\"LT's Strategy to achieve Lowest Immediate Reward:\\n\",LT_Selfish_Plan)\n",
    "print(\"\\nMT's Strategy to achieve Lowest Immediate Reward:\\n\",MT_Selfish_Plan)\n",
    "print(\"\\nST's Strategy to achieve Lowest Immediate Reward:\\n\",ST_Selfish_Plan)\n",
    "\n",
    "#Greedy Policy\n",
    "LT_Greedy = [LT_action_space[i] for i in np.argmin(Q_LT, axis=1)]\n",
    "MT_Greedy = [MT_action_space[i] for i in np.argmin(Q_MT, axis=1)]\n",
    "ST_Greedy = [ST_action_space[i] for i in np.argmin(Q_ST, axis=1)]\n",
    "\n",
    "print(\"\\nGreedy Policy of LT, based on LT's Final Q-Table:\\n\",LT_Greedy)\n",
    "print(\"\\nGreedy Policy of MT, based on MT's Final Q-Table:\\n\",MT_Greedy)\n",
    "print(\"\\nGreedy Policy of ST, based on ST's Final Q-Table:\\n\",ST_Greedy)\n",
    "\n",
    "def get_trend(Q_LT,Q_MT,Q_ST,):\n",
    "    global_state_of_co2_emission = 4.025\n",
    "    global_trend = [4.025]\n",
    "    for year in range(0, years):\n",
    "        #actions based on Policies\n",
    "        LT_action = Q_LT[year]\n",
    "        MT_action = Q_MT[year]\n",
    "        ST_action = Q_ST[year]\n",
    "\n",
    "        global_state_of_co2_emission += (\n",
    "            (LT_action * Population_LT[year]) + (MT_action * Population_MT[year]) +\n",
    "            (ST_action * Population_ST[year])) / Population_array[year]\n",
    "\n",
    "        global_trend.append(np.round(global_state_of_co2_emission,3))\n",
    "    return global_trend\n",
    "\n",
    "#ALL Policies\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(years_array,global_trend_Greedy)\n",
    "plt.scatter(years_array,global_trend_Greedy)\n",
    "plt.plot(years_array,global_trend_Selfish)\n",
    "plt.scatter(years_array,global_trend_Selfish)\n",
    "plt.plot(years_array,global_trend_Synergistic)\n",
    "plt.scatter(years_array,global_trend_Synergistic)\n",
    "plt.plot(years_array,global_trend_Real)\n",
    "plt.scatter(years_array,global_trend_Real)\n",
    "plt.xlabel('Years')\n",
    "plt.ylabel('CO2 Emission')\n",
    "plt.legend(['Greedy','Selfish','Synergistic','Real'])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Source Code_MARL & Global Carbon Footprint_School of AI.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
